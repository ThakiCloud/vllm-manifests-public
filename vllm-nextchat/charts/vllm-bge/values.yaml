replicaCount: 1

image:
  repository: vllm/vllm-openai
  pullPolicy: IfNotPresent
  tag: "v0.9.1"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: "vllm"

serviceAccount:
  create: true
  annotations: {}
  name: "vllm-sa"

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8000"
  prometheus.io/path: "/metrics"

podSecurityContext:
  fsGroup: 2000

securityContext:
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: LoadBalancer
  port: 8000
  targetPort: 8000
  name: "vllm-svc"

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts: []
  tls: []

resources:
  limits:
    memory: "12Gi"
    cpu: "6"
  requests:
    memory: "8Gi"
    cpu: "4"

autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 4
  targetCPUUtilizationPercentage: 80
  # Custom metrics for GPU utilization will be handled by separate HPA manifest

nodeSelector:
  accelerator: nvidia-l40

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - vllm
        topologyKey: kubernetes.io/hostname

# vLLM specific configurations for Qwen3-8B
vllm:
  model: "BAAI/bge-small-en-v1.5"
  host: "0.0.0.0"
  port: 8000
  maxModelLen: 4096
  gpuMemoryUtilization: 0.9
  quantization: null
  dtype: "auto"
  trust_remote_code: true
  tensor_parallel_size: 1
  pipeline_parallel_size: 1

  # OpenAI API compatibility
  served_model_name: "bge-small-en-v1.5"
  chat_template: null

  # Performance tuning for L40
  max_num_seqs: 256
  # max_paddings: 256

env:
  - name: CUDA_VISIBLE_DEVICES
    value: "0"
  - name: TRANSFORMERS_CACHE
    value: "/data/model_cache"
  - name: HF_HOME
    value: "/data/hf_cache"
  - name: TORCH_COMPILE_DISABLE
    value: "1"
  - name: TORCHINDUCTOR_CACHE_DIR
    value: "/tmp/torch_cache"
  - name: USER
    value: "vllm"
  - name: VLLM_PORT
    value: "8000"

# Hugging Face token for model download
hfToken:
  enabled: true
  secretName: "hf-token-secret"
  secretKey: "token"

persistence:
  enabled: true
  accessMode: ReadWriteOnce
  size: 100Gi
  storageClass: ""
  mountPath: /data

modelCache:
  enabled: true
  size: 50Gi
  storageClass: ""
  mountPath: /data/model_cache

# Health checks
livenessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /health
    port: 8000
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
