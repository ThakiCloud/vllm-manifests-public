vllm:
  model: "BAAI/bge-small-en-v1.5"
  maxModelLen: 512  # 임베딩 모델은 보통 짧은 시퀀스 길이
  trust_remote_code: true
  served_model_name: "bge-small-en-v1.5"
  
  # CPU 전용 임베딩 모델 설정
  additionalArgs:
    - "--enforce-eager"  # 즉시 실행 모드
  
  nodeSelector:
    accelerator: nvidia-l40
  
  resources:
    limits:
      memory: "12Gi"
      cpu: "6"
    requests:
      memory: "8Gi"
      cpu: "4"

nextchat:
  baseUrl: "http://vllm:8000"
  openaiApiKey: "sk-proxy"
  defaultModel: "qwen3-8b"
  customModels: "qwen3-8b"
  hideUserApiKey: true
  env:
    - name: DEFAULT_MODEL
      value: "qwen3-8b"
    - name: CUSTOM_MODELS
      value: "qwen3-8b"

hfToken:
  enabled: true

# Persistence 설정 - storage class 지정
persistence:
  enabled: true
  storageClass: "csi-rbd-sc"
  size: 50Gi
  accessMode: ReadWriteOnce

# Model cache 설정 - storage class 지정
modelCache:
  enabled: true
  storageClass: "csi-rbd-sc"
  size: 20Gi
  
ingress:
  enabled: false  # 초기 테스트에서는 비활성화

monitoring:
  enabled: true
  dcgmExporter:
    enabled: true

hpa:
  enabled: true
  minReplicas: 1
  maxReplicas: 3
