replicaCount: 1

image:
  repository: vllm/vllm-openai
  pullPolicy: IfNotPresent
  tag: "v0.9.1"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: "vllm"

serviceAccount:
  create: true
  annotations: {}
  name: "vllm-sa"

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8000"
  prometheus.io/path: "/metrics"

podSecurityContext:
  fsGroup: 2000

securityContext:
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: LoadBalancer
  port: 8000
  targetPort: 8000
  name: "vllm-svc"

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts: []
  tls: []

autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 4
  targetCPUUtilizationPercentage: 80
  # Custom metrics for GPU utilization will be handled by separate HPA manifest

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - vllm
        topologyKey: kubernetes.io/hostname

# vLLM specific configurations for Qwen3-8B
vllm:
  host: "0.0.0.0"
  port: 8000
  maxModelLen: 4096
  gpuMemoryUtilization: 0.9
  quantization: null
  dtype: "auto"
  trust_remote_code: true
  tensor_parallel_size: 1
  pipeline_parallel_size: 1

  chat_template: null

  # Performance tuning for L40
  max_num_seqs: 256
  # max_paddings: 256

env:
  - name: CUDA_VISIBLE_DEVICES
    value: "0"
  - name: TRANSFORMERS_CACHE
    value: "/data/model_cache"
  - name: HF_HOME
    value: "/data/hf_cache"
  - name: TORCH_COMPILE_DISABLE
    value: "1"
  - name: TORCHINDUCTOR_CACHE_DIR
    value: "/tmp/torch_cache"
  - name: USER
    value: "vllm"
  - name: VLLM_PORT
    value: "8000"

# Hugging Face token for model download
hfToken:
  enabled: true
  secretName: "hf-token-secret"
  secretKey: "token"

persistence:
  enabled: true
  accessMode: ReadWriteOnce
  size: 80Gi
  storageClass: ""
  mountPath: /data

modelCache:
  enabled: true
  size: 20Gi
  storageClass: ""
  mountPath: /data/model_cache

# Health checks
#livenessProbe:
#  httpGet:
#    path: /health
#    port: 8000
#  initialDelaySeconds: 600
#  periodSeconds: 30
#  timeoutSeconds: 10
#  failureThreshold: 3

#readinessProbe:
#  httpGet:
#    path: /health
#    port: 8000
#  initialDelaySeconds: 600
#  periodSeconds: 10
#  timeoutSeconds: 5
#  failureThreshold: 3
